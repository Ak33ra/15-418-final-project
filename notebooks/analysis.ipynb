{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20ae549",
   "metadata": {},
   "source": [
    "# Runtime Data Analysis\n",
    "\n",
    "This notebook provides a flexible, generic data analysis framework for processing runtime metrics and performance data.\n",
    "\n",
    "## Features\n",
    "- Load data from CSV or JSON files individually or in bulk\n",
    "- Flexible label extraction for organizing datasets\n",
    "- Comprehensive statistical analysis (mean, std, percentiles, etc.)\n",
    "- Multiple visualization options (histograms, KDE, box plots, bar charts)\n",
    "- Easy to extend for new metrics or data types\n",
    "- Export summaries to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0896fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Union, Optional\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "class DataAnalyzer:\n",
    "    \"\"\"\n",
    "    Generic data analyzer for runtime metrics and other performance data.\n",
    "    Easily extensible for different metrics and file formats.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "        self.metadata = {}\n",
    "    \n",
    "    def load_files(self, \n",
    "                   file_paths: Union[str, List[str]], \n",
    "                   dataset_name: Optional[str] = None,\n",
    "                   label_extractor: Optional[callable] = None) -> None:\n",
    "        \"\"\"\n",
    "        Load data from one or more files.\n",
    "        \n",
    "        Args:\n",
    "            file_paths: Single file path or list of file paths\n",
    "            dataset_name: Name for this dataset (optional, will use filename if not provided)\n",
    "            label_extractor: Optional function to extract label from filename (used when loading multiple files)\n",
    "        \"\"\"\n",
    "        if isinstance(file_paths, str):\n",
    "            file_paths = [file_paths]\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            path = Path(file_path)\n",
    "            \n",
    "            if not path.exists():\n",
    "                print(f\"Warning: {file_path} not found, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Extract label from filename\n",
    "            if label_extractor:\n",
    "                label = label_extractor(path)\n",
    "            elif dataset_name:\n",
    "                label = dataset_name\n",
    "            else:\n",
    "                label = path.stem\n",
    "            \n",
    "            # Load data based on file extension\n",
    "            if path.suffix == '.csv':\n",
    "                df = pd.read_csv(file_path)\n",
    "            elif path.suffix == '.json':\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                df = pd.DataFrame(json_data)\n",
    "            elif path.suffix == '.jsonl':\n",
    "                # Read JSONL (one JSON object per line)\n",
    "                df = pd.read_json(file_path, lines=True)\n",
    "            else:\n",
    "                print(f\"Warning: Unsupported file type {path.suffix}\")\n",
    "                continue\n",
    "            \n",
    "            self.data[label] = df\n",
    "            self.metadata[label] = {\n",
    "                'file_path': str(file_path),\n",
    "                'num_samples': len(df),\n",
    "                'columns': list(df.columns)\n",
    "            }\n",
    "            \n",
    "            print(f\"Loaded {len(df)} samples from {path.name} with columns: {list(df.columns)}\")\n",
    "    \n",
    "    def load_pattern(self, \n",
    "                     pattern: str, \n",
    "                     label_extractor: Optional[callable] = None) -> None:\n",
    "        \"\"\"\n",
    "        Load all files matching a glob pattern.\n",
    "        \n",
    "        Args:\n",
    "            pattern: Glob pattern (e.g., 'experiments/logs/*.csv')\n",
    "            label_extractor: Optional function to extract label from filename\n",
    "        \"\"\"\n",
    "        files = list(Path('.').glob(pattern))\n",
    "        if not files:\n",
    "            print(f\"No files found matching pattern: {pattern}\")\n",
    "            return\n",
    "        \n",
    "        file_paths = [str(f) for f in files]\n",
    "        self.load_files(file_paths, label_extractor=label_extractor)\n",
    "    \n",
    "    def add_throughput_column(self,\n",
    "                              latency_column='latency_ms',\n",
    "                              batch_size=8,\n",
    "                              seq_len=128,\n",
    "                              throughput_column='throughput',\n",
    "                              datasets=None):\n",
    "        \"\"\"\n",
    "        Add a throughput column to the dataframes.\n",
    "        \n",
    "        Throughput = (batch_size * seq_len) / (latency_ms / 1000)\n",
    "        \n",
    "        Args:\n",
    "            latency_column: Column name containing latency data in milliseconds\n",
    "            batch_size: Batch size used in experiment\n",
    "            seq_len: Sequence length (number of tokens per sequence)\n",
    "            throughput_column: Name for the new throughput column\n",
    "            datasets: List of datasets to process (None = all)\n",
    "        \"\"\"\n",
    "        if datasets is None:\n",
    "            datasets = list(self.data.keys())\n",
    "        \n",
    "        tokens_per_batch = batch_size * seq_len\n",
    "        \n",
    "        for dataset_name in datasets:\n",
    "            df = self.data[dataset_name]\n",
    "            \n",
    "            if latency_column not in df.columns:\n",
    "                print(f\"Warning: Column '{latency_column}' not found in dataset '{dataset_name}', skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Convert latency from ms to seconds and compute throughput\n",
    "            latency_seconds = df[latency_column] / 1000.0\n",
    "            df[throughput_column] = tokens_per_batch / latency_seconds\n",
    "            \n",
    "            # Update metadata to include new column\n",
    "            self.metadata[dataset_name]['columns'] = list(df.columns)\n",
    "            \n",
    "            print(f\"Added '{throughput_column}' column to dataset '{dataset_name}'\")\n",
    "        \n",
    "        print(f\"\\nThroughput computed with: batch_size={batch_size}, seq_len={seq_len}, tokens_per_batch={tokens_per_batch}\")\n",
    "    \n",
    "    def get_gpu_cpu_runtime(self, dataset_name: Optional[str] = None, \n",
    "                           events_file: Optional[str] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate total GPU and CPU runtime from events data.\n",
    "        \n",
    "        The GPU time is the sum of all iteration latencies (actual GPU execution time).\n",
    "        The CPU time is the wall time minus GPU time (CPU overhead for scheduling, data transfer, etc.).\n",
    "        \n",
    "        Args:\n",
    "            dataset_name: Name of dataset to analyze (uses loaded data)\n",
    "            events_file: Path to events.jsonl file (alternative to using loaded data)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with timing statistics\n",
    "        \"\"\"\n",
    "        # Load events from file or use loaded dataset\n",
    "        if events_file:\n",
    "            with open(events_file, 'r') as f:\n",
    "                events = [json.loads(line) for line in f]\n",
    "        elif dataset_name and dataset_name in self.data:\n",
    "            # Convert dataframe back to list of dicts\n",
    "            df = self.data[dataset_name]\n",
    "            events = df.to_dict('records')\n",
    "        else:\n",
    "            print(f\"Error: Must provide either dataset_name or events_file\")\n",
    "            return None\n",
    "        \n",
    "        # Parse events\n",
    "        iterations = [e for e in events if e.get('type') == 'iteration']\n",
    "        time_elapsed_event = next((e for e in events if e.get('type') == 'time_elapsed'), None)\n",
    "        run_start = next((e for e in events if e.get('type') == 'run_start'), None)\n",
    "        run_end = next((e for e in events if e.get('type') == 'run_end'), None)\n",
    "        \n",
    "        if not iterations:\n",
    "            print(\"Error: No iteration events found\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate GPU time (sum of all iteration latencies)\n",
    "        total_gpu_time_ms = sum(it['latency_ms'] for it in iterations)\n",
    "        \n",
    "        # Get wall time\n",
    "        if time_elapsed_event and 'sec' in time_elapsed_event:\n",
    "            total_wall_time_ms = time_elapsed_event['sec'] * 1000\n",
    "        elif run_start and run_end:\n",
    "            # Fallback: calculate from timestamps\n",
    "            total_wall_time_ms = (run_end['timestamp'] - run_start['timestamp']) * 1000\n",
    "        else:\n",
    "            print(\"Warning: Could not determine wall time\")\n",
    "            total_wall_time_ms = total_gpu_time_ms  # Fallback\n",
    "        \n",
    "        # Calculate CPU overhead time\n",
    "        total_cpu_time_ms = max(0, total_wall_time_ms - total_gpu_time_ms)\n",
    "        \n",
    "        # Calculate GPU utilization\n",
    "        gpu_utilization_pct = (total_gpu_time_ms / total_wall_time_ms * 100) if total_wall_time_ms > 0 else 0\n",
    "        \n",
    "        results = {\n",
    "            'total_gpu_time_ms': total_gpu_time_ms,\n",
    "            'total_cpu_time_ms': total_cpu_time_ms,\n",
    "            'total_wall_time_ms': total_wall_time_ms,\n",
    "            'num_iterations': len(iterations),\n",
    "            'gpu_utilization_pct': gpu_utilization_pct,\n",
    "            'avg_gpu_time_per_iter_ms': total_gpu_time_ms / len(iterations) if iterations else 0,\n",
    "            'avg_cpu_overhead_per_iter_ms': total_cpu_time_ms / len(iterations) if iterations else 0\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        label = dataset_name if dataset_name else (events_file if events_file else \"Unknown\")\n",
    "        # print(f\"\\nGPU/CPU Runtime Analysis: {label}\")\n",
    "        # print(f\"{'='*70}\")\n",
    "        # print(f\"Total GPU Time:              {total_gpu_time_ms:12.2f} ms\")\n",
    "        # print(f\"Total CPU Overhead Time:     {total_cpu_time_ms:12.2f} ms\")\n",
    "        # print(f\"Total Wall Time:             {total_wall_time_ms:12.2f} ms\")\n",
    "        # print(f\"Number of Iterations:        {len(iterations):12,d}\")\n",
    "        # print(f\"GPU Utilization:             {gpu_utilization_pct:12.2f} %\")\n",
    "        # print(f\"Avg GPU Time per Iteration:  {results['avg_gpu_time_per_iter_ms']:12.2f} ms\")\n",
    "        # print(f\"Avg CPU Overhead per Iter:   {results['avg_cpu_overhead_per_iter_ms']:12.2f} ms\")\n",
    "        # print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compute_statistics(self, column: str, datasets: Optional[List[str]] = None, percentiles: List[float] = [50, 90, 95, 99]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute comprehensive statistics for all loaded datasets.\n",
    "        \n",
    "        Args:\n",
    "            column: Column name to analyze\n",
    "            datasets: List of dataset names to include (None = all)\n",
    "            percentiles: List of percentiles to compute\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with statistics for each dataset\n",
    "        \"\"\"\n",
    "        if datasets is None:\n",
    "            datasets = list(self.data.keys())\n",
    "            \n",
    "        stats_list = []\n",
    "        \n",
    "        for label in datasets:\n",
    "            df = self.data[label]\n",
    "            \n",
    "            if column not in df.columns:\n",
    "                print(f\"Warning: Column '{column}' not found in dataset '{label}', skipping...\")\n",
    "                continue\n",
    "                \n",
    "            values = df[column]\n",
    "            \n",
    "            stats = {\n",
    "                'dataset': label,\n",
    "                'column': column,\n",
    "                'count': len(values),\n",
    "                'mean': values.mean(),\n",
    "                'std': values.std(),\n",
    "                'min': values.min(),\n",
    "                'max': values.max(),\n",
    "            }\n",
    "            \n",
    "            # Add percentiles\n",
    "            for p in percentiles:\n",
    "                stats[f'p{p}'] = values.quantile(p/100)\n",
    "            \n",
    "            stats_list.append(stats)\n",
    "        \n",
    "        return pd.DataFrame(stats_list)\n",
    "    \n",
    "    def plot_distributions(self, \n",
    "                          column: str,\n",
    "                          datasets: Optional[List[str]] = None,\n",
    "                          plot_type: str = 'hist',\n",
    "                          bins: int = 50,\n",
    "                          figsize: tuple = (14, 6)) -> None:\n",
    "        \"\"\"\n",
    "        Plot distributions of metrics with equal-width bins for consistent visualization.\n",
    "        \n",
    "        Args:\n",
    "            column: Column name to analyze\n",
    "            datasets: List of dataset labels to plot (None = all)\n",
    "            plot_type: 'hist', 'kde', or 'both'\n",
    "            bins: Number of bins for histogram (or bin edges array)\n",
    "            figsize: Figure size\n",
    "        \"\"\"\n",
    "        if datasets is None:\n",
    "            datasets = list(self.data.keys())\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Collect all data to determine global min/max for equal-width bins\n",
    "        all_values = []\n",
    "        for label in datasets:\n",
    "            df = self.data[label]\n",
    "            if column in df.columns:\n",
    "                all_values.extend(df[column].dropna().values)\n",
    "        \n",
    "        if len(all_values) == 0:\n",
    "            print(f\"Warning: No data found for column '{column}'\")\n",
    "            return\n",
    "        \n",
    "        # Create equal-width bin edges based on ALL data\n",
    "        if isinstance(bins, int):\n",
    "            bin_edges = np.linspace(min(all_values), max(all_values), bins + 1)\n",
    "        else:\n",
    "            # If bins is already an array, use it as-is\n",
    "            bin_edges = bins\n",
    "        \n",
    "        for label in datasets:\n",
    "            df = self.data[label]\n",
    "            \n",
    "            if column not in df.columns:\n",
    "                print(f\"Warning: Column '{column}' not found in dataset '{label}', skipping...\")\n",
    "                continue\n",
    "                \n",
    "            values = df[column]\n",
    "            \n",
    "            # Histogram with equal-width bins\n",
    "            if plot_type in ['hist', 'both']:\n",
    "                axes[0].hist(values, bins=bin_edges, alpha=0.6, label=label)\n",
    "            \n",
    "            # KDE\n",
    "            if plot_type in ['kde', 'both']:\n",
    "                values.plot.kde(ax=axes[1], label=label)\n",
    "        \n",
    "        axes[0].set_xlabel(f'{column}')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        axes[0].set_title('Distribution (Histogram)')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1].set_xlabel(f'{column}')\n",
    "        axes[1].set_ylabel('Density')\n",
    "        axes[1].set_title('Distribution (KDE)')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_comparison(self,\n",
    "                       dataset: str,\n",
    "                       column: str,\n",
    "                       metrics: List[str] = ['mean', 'median', 'p95', 'p99'],\n",
    "                       figsize: tuple = (10, 6)) -> None:\n",
    "        \"\"\"\n",
    "        Bar plot comparing multiple metrics for a single dataset and column.\n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset label to analyze\n",
    "            column: Column name to analyze\n",
    "            metrics: List of metrics to compare ('mean', 'median', 'p95', etc.)\n",
    "            figsize: Figure size\n",
    "        \"\"\"\n",
    "        if dataset not in self.data:\n",
    "            print(f\"Error: Dataset '{dataset}' not found\")\n",
    "            print(f\"Available datasets: {self.list_datasets()}\")\n",
    "            return\n",
    "\n",
    "        df = self.data[dataset]\n",
    "        \n",
    "        if column not in df.columns:\n",
    "            print(f\"Error: Column '{column}' not found in dataset '{dataset}'\")\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "            return\n",
    "            \n",
    "        data = df[column]\n",
    "\n",
    "        values = []\n",
    "        labels = []\n",
    "\n",
    "        for metric in metrics:\n",
    "            if metric == 'mean':\n",
    "                val = data.mean()\n",
    "            elif metric == 'median':\n",
    "                val = data.median()\n",
    "            elif metric.startswith('p'):\n",
    "                percentile = int(metric[1:])\n",
    "                val = data.quantile(percentile/100)\n",
    "            elif metric in ['min', 'max', 'std']:\n",
    "                val = getattr(data, metric)()\n",
    "            else:\n",
    "                print(f\"Warning: Unknown metric '{metric}', skipping\")\n",
    "                continue\n",
    "\n",
    "            values.append(val)\n",
    "            labels.append(metric.upper())\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.bar(labels, values)\n",
    "        plt.xlabel('Metric')\n",
    "        plt.ylabel(f'{column}')\n",
    "        plt.title(f'Metric Comparison for {dataset} - {column}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_boxplot(self, \n",
    "                     column: str,\n",
    "                     datasets: Optional[List[str]] = None,\n",
    "                     figsize: tuple = (12, 6)) -> None:\n",
    "        \"\"\"\n",
    "        Create box plot for comparing distributions.\n",
    "        \n",
    "        Args:\n",
    "            column: Column name to analyze\n",
    "            datasets: List of dataset labels to plot (None = all)\n",
    "            figsize: Figure size\n",
    "        \"\"\"\n",
    "        if datasets is None:\n",
    "            datasets = list(self.data.keys())\n",
    "        \n",
    "        plot_data = []\n",
    "        plot_labels = []\n",
    "        \n",
    "        for label in datasets:\n",
    "            df = self.data[label]\n",
    "            \n",
    "            if column not in df.columns:\n",
    "                print(f\"Warning: Column '{column}' not found in dataset '{label}', skipping...\")\n",
    "                continue\n",
    "                \n",
    "            plot_data.append(df[column].values)\n",
    "            plot_labels.append(label)\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.boxplot(plot_data, labels=plot_labels)\n",
    "        plt.xlabel('Dataset')\n",
    "        plt.ylabel(f'{column}')\n",
    "        plt.title(f'Distribution Comparison (Box Plot) - {column}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def export_summary(self, column: str, output_file: str, datasets: Optional[List[str]] = None, percentiles: List[float] = [50, 90, 95, 99]) -> None:\n",
    "        \"\"\"\n",
    "        Export summary statistics to CSV.\n",
    "        \n",
    "        Args:\n",
    "            column: Column name to analyze\n",
    "            output_file: Path to output CSV file\n",
    "            datasets: List of dataset names to include (None = all)\n",
    "            percentiles: List of percentiles to include\n",
    "        \"\"\"\n",
    "        stats_df = self.compute_statistics(column, datasets, percentiles)\n",
    "        stats_df.to_csv(output_file, index=False)\n",
    "        print(f\"Summary exported to {output_file}\")\n",
    "    \n",
    "    def get_data(self, label: str) -> pd.DataFrame:\n",
    "        \"\"\"Get raw data for a specific dataset.\"\"\"\n",
    "        return self.data.get(label)\n",
    "    \n",
    "    def list_datasets(self) -> List[str]:\n",
    "        \"\"\"List all loaded datasets.\"\"\"\n",
    "        return list(self.data.keys())\n",
    "    \n",
    "    def get_columns(self, dataset: str) -> List[str]:\n",
    "        \"\"\"Get list of columns for a specific dataset.\"\"\"\n",
    "        if dataset in self.metadata:\n",
    "            return self.metadata[dataset]['columns']\n",
    "        return []\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear all loaded data.\"\"\"\n",
    "        self.data.clear()\n",
    "        self.metadata.clear()\n",
    "\n",
    "# Helper functions for common label extraction patterns\n",
    "def extract_batch_size(filename: str) -> str:\n",
    "    \"\"\"Extract batch size from filename like 'model_b8_L128_latencies_ms.csv'\"\"\"\n",
    "    match = re.search(r'_b(\\d+)_', filename)\n",
    "    return f\"batch_{match.group(1)}\" if match else filename\n",
    "\n",
    "def extract_model_and_batch(filename: str) -> str:\n",
    "    \"\"\"Extract model and batch size from filename\"\"\"\n",
    "    model_match = re.search(r'(distilgpt2|Mistral)', filename)\n",
    "    batch_match = re.search(r'_b(\\d+)_', filename)\n",
    "    \n",
    "    model = model_match.group(1) if model_match else \"unknown\"\n",
    "    batch = batch_match.group(1) if batch_match else \"unknown\"\n",
    "    \n",
    "    return f\"{model}_b{batch}\"\n",
    "\n",
    "print(\"DataAnalyzer class loaded successfully!\")\n",
    "print(\"\\nQuick start examples:\")\n",
    "print(\"  analyzer = DataAnalyzer()\")\n",
    "print(\"  analyzer.load_files('path/to/file.csv', dataset_name='experiment1')\")\n",
    "print(\"  analyzer.load_pattern('experiments/logs/*.csv', label_extractor=extract_batch_size)\")\n",
    "print(\"  stats = analyzer.compute_statistics(column='latency_ms')\")\n",
    "print(\"  analyzer.plot_distributions(column='latency_ms')\")\n",
    "print(\"  analyzer.plot_comparison(dataset='experiment1', column='latency_ms', metrics=['mean', 'p95', 'p99'])\")\n",
    "print(\"\\n  # Get GPU/CPU runtime breakdown:\")\n",
    "print(\"  gpu_cpu_stats = analyzer.get_gpu_cpu_runtime(dataset_name='experiment1')\")\n",
    "print(\"  # Or from a file:\")\n",
    "print(\"  gpu_cpu_stats = analyzer.get_gpu_cpu_runtime(events_file='path/to/events.jsonl')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9j422dcsari",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add method to DataAnalyzer for computing total throughput\n",
    "def compute_total_throughput(analyzer,\n",
    "                              latency_column='latency_ms',\n",
    "                              batch_size=8,\n",
    "                              seq_len=128,\n",
    "                              datasets=None):\n",
    "    \"\"\"\n",
    "    Compute total throughput for multi-tenant experiments.\n",
    "    \n",
    "    For multi-tenant scenarios, the total throughput is the sum of individual\n",
    "    model throughputs. This allows comparison between:\n",
    "    - Single model on 1 GPU\n",
    "    - Multiple models sharing 1 GPU\n",
    "    \n",
    "    Args:\n",
    "        analyzer: DataAnalyzer instance\n",
    "        latency_column: Column name containing latency data in milliseconds\n",
    "        batch_size: Batch size used in experiment\n",
    "        seq_len: Sequence length (number of tokens per sequence)\n",
    "        datasets: List of datasets to include (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with per-iteration total throughput and statistics\n",
    "    \"\"\"\n",
    "    if datasets is None:\n",
    "        datasets = analyzer.list_datasets()\n",
    "    \n",
    "    tokens_per_batch = batch_size * seq_len\n",
    "    \n",
    "    # Collect all throughput data aligned by iteration\n",
    "    throughput_by_iter = {}\n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        df = analyzer.get_data(dataset_name)\n",
    "        \n",
    "        if df is None:\n",
    "            print(f\"Warning: Dataset '{dataset_name}' not found, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        if latency_column not in df.columns:\n",
    "            print(f\"Warning: Column '{latency_column}' not found in dataset '{dataset_name}', skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Compute throughput for each iteration\n",
    "        for idx, row in df.iterrows():\n",
    "            iter_num = row.get('iter', idx)\n",
    "            latency_ms = row[latency_column]\n",
    "            throughput = tokens_per_batch / (latency_ms / 1000.0)\n",
    "            \n",
    "            if iter_num not in throughput_by_iter:\n",
    "                throughput_by_iter[iter_num] = []\n",
    "            throughput_by_iter[iter_num].append(throughput)\n",
    "    \n",
    "    # Sum throughputs for each iteration\n",
    "    total_throughput_data = []\n",
    "    for iter_num in sorted(throughput_by_iter.keys()):\n",
    "        total_throughput = sum(throughput_by_iter[iter_num])\n",
    "        total_throughput_data.append({\n",
    "            'iter': iter_num,\n",
    "            'total_throughput': total_throughput,\n",
    "            'num_models': len(throughput_by_iter[iter_num])\n",
    "        })\n",
    "    \n",
    "    result_df = pd.DataFrame(total_throughput_data)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TOTAL THROUGHPUT ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Configuration: {len(datasets)} models on 1 GPU\")\n",
    "    print(f\"Batch size: {batch_size}, Seq length: {seq_len}\")\n",
    "    print(f\"Tokens per batch: {tokens_per_batch}\")\n",
    "    print(f\"\\nTotal Throughput Statistics (tokens/second):\")\n",
    "    print(f\"  Mean:   {result_df['total_throughput'].mean():,.2f}\")\n",
    "    print(f\"  Std:    {result_df['total_throughput'].std():,.2f}\")\n",
    "    print(f\"  Min:    {result_df['total_throughput'].min():,.2f}\")\n",
    "    print(f\"  Max:    {result_df['total_throughput'].max():,.2f}\")\n",
    "    print(f\"  Median: {result_df['total_throughput'].median():,.2f}\")\n",
    "    print(f\"  P95:    {result_df['total_throughput'].quantile(0.95):,.2f}\")\n",
    "    print(f\"  P99:    {result_df['total_throughput'].quantile(0.99):,.2f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def compare_multi_vs_single_tenant(multi_tenant_analyzer, \n",
    "                                    single_tenant_analyzer,\n",
    "                                    batch_size=8,\n",
    "                                    seq_len=128,\n",
    "                                    figsize=(14, 6)):\n",
    "    \"\"\"\n",
    "    Compare total throughput between multi-tenant and single-tenant configurations.\n",
    "    \n",
    "    Args:\n",
    "        multi_tenant_analyzer: DataAnalyzer with multiple model datasets loaded\n",
    "        single_tenant_analyzer: DataAnalyzer with single model dataset loaded\n",
    "        batch_size: Batch size used\n",
    "        seq_len: Sequence length\n",
    "        figsize: Figure size for plots\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with comparison statistics\n",
    "    \"\"\"\n",
    "    # Compute total throughput for multi-tenant\n",
    "    print(\"Computing multi-tenant total throughput...\")\n",
    "    multi_df = compute_total_throughput(multi_tenant_analyzer, \n",
    "                                       batch_size=batch_size, \n",
    "                                       seq_len=seq_len)\n",
    "    \n",
    "    # Compute throughput for single tenant\n",
    "    print(\"\\nComputing single-tenant throughput...\")\n",
    "    single_datasets = single_tenant_analyzer.list_datasets()\n",
    "    if len(single_datasets) == 0:\n",
    "        print(\"Error: No datasets in single_tenant_analyzer\")\n",
    "        return None\n",
    "    \n",
    "    single_df = single_tenant_analyzer.get_data(single_datasets[0])\n",
    "    tokens_per_batch = batch_size * seq_len\n",
    "    single_throughput = tokens_per_batch / (single_df['latency_ms'] / 1000.0)\n",
    "    \n",
    "    # Comparison statistics\n",
    "    multi_mean = multi_df['total_throughput'].mean()\n",
    "    single_mean = single_throughput.mean()\n",
    "    improvement = ((multi_mean - single_mean) / single_mean) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MULTI-TENANT vs SINGLE-TENANT COMPARISON\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Multi-tenant ({multi_df['num_models'].iloc[0]} models):\")\n",
    "    print(f\"  Mean throughput: {multi_mean:,.2f} tokens/s\")\n",
    "    print(f\"\\nSingle-tenant (1 model):\")\n",
    "    print(f\"  Mean throughput: {single_mean:,.2f} tokens/s\")\n",
    "    print(f\"\\nDifference: {improvement:+.2f}%\")\n",
    "    if improvement > 0:\n",
    "        print(f\"Multi-tenant provides {improvement:.2f}% MORE total throughput\")\n",
    "    else:\n",
    "        print(f\"Multi-tenant provides {abs(improvement):.2f}% LESS total throughput\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Distribution comparison\n",
    "    axes[0].hist(multi_df['total_throughput'], bins=30, alpha=0.6, label=f'Multi-tenant ({multi_df[\"num_models\"].iloc[0]} models)', color='blue')\n",
    "    axes[0].hist(single_throughput, bins=30, alpha=0.6, label='Single-tenant (1 model)', color='orange')\n",
    "    axes[0].set_xlabel('Throughput (tokens/s)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Throughput Distribution Comparison')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bar chart of means\n",
    "    means = [multi_mean, single_mean]\n",
    "    labels = [f'Multi-tenant\\n({multi_df[\"num_models\"].iloc[0]} models)', 'Single-tenant\\n(1 model)']\n",
    "    colors = ['blue', 'orange']\n",
    "    \n",
    "    bars = axes[1].bar(labels, means, color=colors, alpha=0.6)\n",
    "    axes[1].set_ylabel('Mean Throughput (tokens/s)')\n",
    "    axes[1].set_title('Mean Throughput Comparison')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:,.0f}',\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'multi_tenant_mean': multi_mean,\n",
    "        'single_tenant_mean': single_mean,\n",
    "        'improvement_percent': improvement,\n",
    "        'multi_df': multi_df,\n",
    "        'single_throughput': single_throughput\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Total throughput analysis functions loaded!\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"  # Compute total throughput for multi-tenant setup\")\n",
    "print(\"  total_throughput_df = compute_total_throughput(analyzer, batch_size=8, seq_len=128)\")\n",
    "print(\"\")\n",
    "print(\"  # Compare multi-tenant vs single-tenant\")\n",
    "print(\"  comparison = compare_multi_vs_single_tenant(multi_analyzer, single_analyzer, batch_size=8, seq_len=128)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yo6ahgr0zu",
   "metadata": {},
   "source": [
    "## Statistical Fairness & Significance Analysis\n",
    "\n",
    "This section provides tools to analyze:\n",
    "1. **Fairness**: Whether GPU resources are shared fairly among models\n",
    "2. **Statistical Significance**: Whether performance differences are statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0gj7hp3qs725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from typing import Tuple\n",
    "\n",
    "def compute_fairness_metrics(analyzer, column='latency_ms', datasets=None, metric='mean', quantiles=[0.25, 0.5, 0.75]):\n",
    "    \"\"\"\n",
    "    Compute fairness metrics for resource sharing among models.\n",
    "    \n",
    "    Fairness Metrics:\n",
    "    - Coefficient of Variation (CV): std/metric_val, lower is fairer (0 = perfectly fair)\n",
    "    - Gini Coefficient: 0 (perfect equality) to 1 (perfect inequality)\n",
    "    - Max/Min Ratio: how many times slower is the slowest vs fastest\n",
    "    - Range: difference between max and min\n",
    "    - Mann-Whitney U Tests: Pairwise comparisons based on median differences\n",
    "    - Quantile Regression: Analyze fairness across different percentiles\n",
    "    \n",
    "    Args:\n",
    "        analyzer: DataAnalyzer instance\n",
    "        column: Column to analyze (default 'latency_ms')\n",
    "        datasets: List of datasets (None = all)\n",
    "        metric: Metric to use ('mean', 'median', or 'p50') - default 'mean'\n",
    "        quantiles: List of quantiles for quantile regression analysis\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with fairness metrics\n",
    "    \"\"\"\n",
    "    if datasets is None:\n",
    "        datasets = analyzer.list_datasets()\n",
    "    \n",
    "    # Get metric values for each model\n",
    "    metric_values = []\n",
    "    medians = []\n",
    "    data_by_model = []  # Store full data for Mann-Whitney tests\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        df = analyzer.get_data(dataset)\n",
    "        if df is not None and column in df.columns:\n",
    "            data = df[column].dropna().values\n",
    "            data_by_model.append(data)\n",
    "            medians.append(np.median(data))\n",
    "            \n",
    "            if metric == 'mean':\n",
    "                metric_values.append(np.mean(data))\n",
    "            elif metric in ['median', 'p50']:\n",
    "                metric_values.append(np.median(data))\n",
    "            else:\n",
    "                # Try to parse as percentile (e.g., 'p95')\n",
    "                if metric.startswith('p'):\n",
    "                    percentile = int(metric[1:])\n",
    "                    metric_values.append(np.percentile(data, percentile))\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown metric: {metric}\")\n",
    "    \n",
    "    if len(metric_values) == 0:\n",
    "        print(\"Error: No valid data found\")\n",
    "        return None\n",
    "    \n",
    "    metric_values = np.array(metric_values)\n",
    "    medians = np.array(medians)\n",
    "    \n",
    "    # ========== Traditional Fairness Metrics ==========\n",
    "    # Coefficient of Variation (lower is better, 0 = perfectly fair)\n",
    "    cv = np.std(metric_values) / np.mean(metric_values)\n",
    "    \n",
    "    # Gini Coefficient\n",
    "    sorted_values = np.sort(metric_values)\n",
    "    n = len(sorted_values)\n",
    "    gini = (2 * np.sum((np.arange(1, n+1)) * sorted_values)) / (n * np.sum(sorted_values)) - (n + 1) / n\n",
    "    \n",
    "    # Max/Min ratio\n",
    "    max_min_ratio = np.max(metric_values) / np.min(metric_values)\n",
    "    \n",
    "    # Range\n",
    "    range_val = np.max(metric_values) - np.min(metric_values)\n",
    "    \n",
    "    # Percent difference from mean\n",
    "    mean_val = np.mean(metric_values)\n",
    "    max_deviation_pct = np.max(np.abs(metric_values - mean_val)) / mean_val * 100\n",
    "    \n",
    "    # ========== Mann-Whitney U Tests (Pairwise Median Comparisons) ==========\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FAIRNESS ANALYSIS: {column} (using {metric})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Number of models: {len(metric_values)}\")\n",
    "    print(f\"\\nIndividual {metric}s:\")\n",
    "    for dataset, val in zip(datasets, metric_values):\n",
    "        deviation = ((val - mean_val) / mean_val) * 100\n",
    "        print(f\"  {dataset:20s}: {val:8.2f} ({deviation:+.2f}% from avg)\")\n",
    "    \n",
    "    # Mann-Whitney U Tests (non-parametric test for median differences)\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"MANN-WHITNEY U TESTS (Pairwise Median Comparisons)\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    print(f\"Tests whether the medians of two groups are significantly different.\")\n",
    "    print(f\"More robust to outliers than t-tests.\\n\")\n",
    "    \n",
    "    mann_whitney_results = []\n",
    "    n_comparisons = len(datasets) * (len(datasets) - 1) // 2\n",
    "    bonferroni_alpha = 0.05 / n_comparisons\n",
    "    \n",
    "    for i in range(len(datasets)):\n",
    "        for j in range(i+1, len(datasets)):\n",
    "            # Mann-Whitney U test\n",
    "            u_stat, p_val = stats.mannwhitneyu(data_by_model[i], data_by_model[j], \n",
    "                                               alternative='two-sided')\n",
    "            \n",
    "            median_i = medians[i]\n",
    "            median_j = medians[j]\n",
    "            diff = median_i - median_j\n",
    "            diff_pct = (diff / median_j) * 100\n",
    "            \n",
    "            # Effect size (rank-biserial correlation)\n",
    "            n1, n2 = len(data_by_model[i]), len(data_by_model[j])\n",
    "            r = 1 - (2*u_stat) / (n1 * n2)  # rank-biserial correlation\n",
    "            \n",
    "            is_significant = p_val < bonferroni_alpha\n",
    "            \n",
    "            mann_whitney_results.append({\n",
    "                'model_1': datasets[i],\n",
    "                'model_2': datasets[j],\n",
    "                'median_1': median_i,\n",
    "                'median_2': median_j,\n",
    "                'difference': diff,\n",
    "                'diff_percent': diff_pct,\n",
    "                'u_statistic': u_stat,\n",
    "                'p_value': p_val,\n",
    "                'effect_size_r': r,\n",
    "                'significant': is_significant\n",
    "            })\n",
    "    \n",
    "    # Print Mann-Whitney results\n",
    "    significant_mw = [r for r in mann_whitney_results if r['significant']]\n",
    "    print(f\"Bonferroni-corrected alpha: {bonferroni_alpha:.6f} ({n_comparisons} comparisons)\")\n",
    "    print(f\"\\nPairwise Results:\")\n",
    "    for result in mann_whitney_results:\n",
    "        sig_marker = \"***\" if result['significant'] else \"   \"\n",
    "        print(f\"{sig_marker} {result['model_1']:15s} vs {result['model_2']:15s}: \" +\n",
    "              f\"Δmedian={result['difference']:7.2f} ({result['diff_percent']:+6.2f}%), \" +\n",
    "              f\"p={result['p_value']:.6f}, r={result['effect_size_r']:+.3f}\")\n",
    "    \n",
    "    print(f\"\\nEffect Size Interpretation (rank-biserial r):\")\n",
    "    print(f\"  < 0.1: negligible, 0.1-0.3: small, 0.3-0.5: medium, > 0.5: large\")\n",
    "    \n",
    "    if significant_mw:\n",
    "        print(f\"\\n✓ {len(significant_mw)} significant median difference(s) found:\")\n",
    "        for r in significant_mw:\n",
    "            print(f\"  • {r['model_1']} vs {r['model_2']}: {r['diff_percent']:+.2f}% (p={r['p_value']:.6f})\")\n",
    "    else:\n",
    "        print(f\"\\n✓ No statistically significant median differences found.\")\n",
    "        print(f\"  → Resource sharing appears fair from a statistical perspective.\")\n",
    "    \n",
    "    # ========== Quantile Regression Analysis ==========\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"QUANTILE FAIRNESS ANALYSIS\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    print(f\"Analyzes fairness across different percentiles of the distribution.\")\n",
    "    print(f\"Helps identify if unfairness is concentrated in tail latencies.\\n\")\n",
    "    \n",
    "    quantile_fairness = {}\n",
    "    for q in quantiles:\n",
    "        q_values = []\n",
    "        for data in data_by_model:\n",
    "            q_values.append(np.quantile(data, q))\n",
    "        \n",
    "        q_values = np.array(q_values)\n",
    "        q_cv = np.std(q_values) / np.mean(q_values)\n",
    "        q_max_min = np.max(q_values) / np.min(q_values)\n",
    "        q_range = np.max(q_values) - np.min(q_values)\n",
    "        \n",
    "        quantile_fairness[q] = {\n",
    "            'values': q_values,\n",
    "            'cv': q_cv,\n",
    "            'max_min_ratio': q_max_min,\n",
    "            'range': q_range,\n",
    "            'mean': np.mean(q_values)\n",
    "        }\n",
    "        \n",
    "        print(f\"Q{int(q*100):02d} (p{int(q*100)}): CV={q_cv:.4f}, Max/Min={q_max_min:.3f}x, \" +\n",
    "              f\"Range={q_range:.2f}\")\n",
    "    \n",
    "    print(f\"\\nQuantile Fairness Interpretation:\")\n",
    "    if all(quantile_fairness[q]['cv'] < 0.1 for q in quantiles):\n",
    "        print(f\"  ✓ EXCELLENT - Fair across all percentiles\")\n",
    "    elif all(quantile_fairness[q]['cv'] < 0.2 for q in quantiles):\n",
    "        print(f\"  ✓ GOOD - Reasonably fair across distribution\")\n",
    "    else:\n",
    "        # Check if tail is worse\n",
    "        tail_cv = quantile_fairness[max(quantiles)]['cv']\n",
    "        median_cv = quantile_fairness[0.5]['cv']\n",
    "        if tail_cv > median_cv * 1.5:\n",
    "            print(f\"  ⚠ TAIL UNFAIRNESS - Higher unfairness in tail latencies (p{int(max(quantiles)*100)})\")\n",
    "        else:\n",
    "            print(f\"  ~ MODERATE - Some unfairness across distribution\")\n",
    "    \n",
    "    # ========== Traditional Fairness Metrics Summary ==========\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"TRADITIONAL FAIRNESS METRICS (based on {metric})\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    print(f\"  Coefficient of Variation: {cv:.4f}\")\n",
    "    print(f\"    → Lower is fairer. <0.1 is good, <0.05 is excellent\")\n",
    "    print(f\"  Gini Coefficient: {gini:.4f}\")\n",
    "    print(f\"    → 0 = perfect equality, 1 = perfect inequality\")\n",
    "    print(f\"  Max/Min Ratio: {max_min_ratio:.3f}x\")\n",
    "    print(f\"    → Slowest model is {max_min_ratio:.3f}x slower than fastest\")\n",
    "    print(f\"  Max Deviation: {max_deviation_pct:.2f}%\")\n",
    "    print(f\"    → Worst-case deviation from average\")\n",
    "    \n",
    "    # Fairness assessment\n",
    "    print(f\"\\nOverall Fairness Assessment:\")\n",
    "    if cv < 0.05:\n",
    "        print(f\"  ✓ EXCELLENT - Resources are shared very fairly\")\n",
    "    elif cv < 0.1:\n",
    "        print(f\"  ✓ GOOD - Resources are shared fairly\")\n",
    "    elif cv < 0.2:\n",
    "        print(f\"  ~ MODERATE - Some unfairness in resource sharing\")\n",
    "    else:\n",
    "        print(f\"  ✗ POOR - Significant unfairness in resource sharing\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'num_models': len(metric_values),\n",
    "        'metric': metric,\n",
    "        'mean_of_metrics': mean_val,\n",
    "        'std_of_metrics': np.std(metric_values),\n",
    "        'coefficient_of_variation': cv,\n",
    "        'gini_coefficient': gini,\n",
    "        'max_min_ratio': max_min_ratio,\n",
    "        'range': range_val,\n",
    "        'max_deviation_percent': max_deviation_pct,\n",
    "        'individual_values': {dataset: val for dataset, val in zip(datasets, metric_values)},\n",
    "        'mann_whitney_tests': mann_whitney_results,\n",
    "        'mann_whitney_significant': significant_mw,\n",
    "        'quantile_fairness': quantile_fairness,\n",
    "        'bonferroni_alpha': bonferroni_alpha\n",
    "    }\n",
    "\n",
    "\n",
    "def test_statistical_significance(analyzer, column='latency_ms', datasets=None, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Test whether performance differences between models are statistically significant.\n",
    "    \n",
    "    Tests:\n",
    "    1. One-way ANOVA: Tests if ANY groups differ\n",
    "    2. Pairwise t-tests: Tests each pair of models\n",
    "    3. Levene's test: Tests if variances are equal (homoscedasticity)\n",
    "    \n",
    "    Args:\n",
    "        analyzer: DataAnalyzer instance\n",
    "        column: Column to analyze\n",
    "        datasets: List of datasets (None = all)\n",
    "        alpha: Significance level (default 0.05)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with test results\n",
    "    \"\"\"\n",
    "    if datasets is None:\n",
    "        datasets = analyzer.list_datasets()\n",
    "    \n",
    "    # Get data for each model\n",
    "    data_by_model = []\n",
    "    valid_datasets = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        df = analyzer.get_data(dataset)\n",
    "        if df is not None and column in df.columns:\n",
    "            data_by_model.append(df[column].values)\n",
    "            valid_datasets.append(dataset)\n",
    "    \n",
    "    if len(data_by_model) < 2:\n",
    "        print(\"Error: Need at least 2 datasets for significance testing\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"STATISTICAL SIGNIFICANCE ANALYSIS: {column}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Significance level (alpha): {alpha}\")\n",
    "    print(f\"Number of models: {len(valid_datasets)}\")\n",
    "    \n",
    "    # 1. Levene's test for equal variances\n",
    "    print(f\"\\n1. Levene's Test (Equal Variances):\")\n",
    "    levene_stat, levene_p = stats.levene(*data_by_model)\n",
    "    print(f\"   Statistic: {levene_stat:.4f}, p-value: {levene_p:.4f}\")\n",
    "    if levene_p > alpha:\n",
    "        print(f\"   → Variances are equal (homoscedastic) ✓\")\n",
    "        equal_var = True\n",
    "    else:\n",
    "        print(f\"   → Variances are NOT equal (heteroscedastic) ✗\")\n",
    "        equal_var = False\n",
    "    \n",
    "    # 2. One-way ANOVA (or Kruskal-Wallis if variances unequal)\n",
    "    if equal_var:\n",
    "        print(f\"\\n2. One-way ANOVA:\")\n",
    "        f_stat, anova_p = stats.f_oneway(*data_by_model)\n",
    "        print(f\"   F-statistic: {f_stat:.4f}, p-value: {anova_p:.6f}\")\n",
    "        test_name = \"ANOVA\"\n",
    "    else:\n",
    "        print(f\"\\n2. Kruskal-Wallis Test (non-parametric alternative):\")\n",
    "        h_stat, anova_p = stats.kruskal(*data_by_model)\n",
    "        print(f\"   H-statistic: {h_stat:.4f}, p-value: {anova_p:.6f}\")\n",
    "        test_name = \"Kruskal-Wallis\"\n",
    "    \n",
    "    if anova_p < alpha:\n",
    "        print(f\"   → At least one model is significantly different ✓\")\n",
    "        print(f\"   → p < {alpha}, reject null hypothesis\")\n",
    "    else:\n",
    "        print(f\"   → No significant differences detected\")\n",
    "        print(f\"   → p >= {alpha}, fail to reject null hypothesis\")\n",
    "    \n",
    "    # 3. Pairwise comparisons (only if ANOVA shows significance)\n",
    "    print(f\"\\n3. Pairwise Comparisons (t-tests with Bonferroni correction):\")\n",
    "    n_comparisons = len(valid_datasets) * (len(valid_datasets) - 1) // 2\n",
    "    bonferroni_alpha = alpha / n_comparisons\n",
    "    print(f\"   Bonferroni-corrected alpha: {bonferroni_alpha:.6f} ({n_comparisons} comparisons)\")\n",
    "    \n",
    "    pairwise_results = []\n",
    "    significant_pairs = []\n",
    "    \n",
    "    for i in range(len(valid_datasets)):\n",
    "        for j in range(i+1, len(valid_datasets)):\n",
    "            # Use Welch's t-test if variances are unequal\n",
    "            t_stat, p_val = stats.ttest_ind(data_by_model[i], data_by_model[j], \n",
    "                                           equal_var=equal_var)\n",
    "            \n",
    "            mean_i = np.mean(data_by_model[i])\n",
    "            mean_j = np.mean(data_by_model[j])\n",
    "            diff = mean_i - mean_j\n",
    "            diff_pct = (diff / mean_j) * 100\n",
    "            \n",
    "            # Cohen's d (effect size)\n",
    "            pooled_std = np.sqrt((np.var(data_by_model[i]) + np.var(data_by_model[j])) / 2)\n",
    "            cohens_d = diff / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            is_significant = p_val < bonferroni_alpha\n",
    "            \n",
    "            pairwise_results.append({\n",
    "                'model_1': valid_datasets[i],\n",
    "                'model_2': valid_datasets[j],\n",
    "                'mean_1': mean_i,\n",
    "                'mean_2': mean_j,\n",
    "                'difference': diff,\n",
    "                'diff_percent': diff_pct,\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_val,\n",
    "                'cohens_d': cohens_d,\n",
    "                'significant': is_significant\n",
    "            })\n",
    "            \n",
    "            if is_significant:\n",
    "                significant_pairs.append((valid_datasets[i], valid_datasets[j], p_val, diff_pct))\n",
    "    \n",
    "    # Print pairwise results\n",
    "    print(f\"\\n   Pairwise Test Results:\")\n",
    "    for result in pairwise_results:\n",
    "        sig_marker = \"***\" if result['significant'] else \"   \"\n",
    "        print(f\"   {sig_marker} {result['model_1']:15s} vs {result['model_2']:15s}: \" +\n",
    "              f\"diff={result['difference']:7.2f} ({result['diff_percent']:+6.2f}%), \" +\n",
    "              f\"p={result['p_value']:.6f}, d={result['cohens_d']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n   Effect Size Interpretation (Cohen's d):\")\n",
    "    print(f\"     < 0.2: negligible, 0.2-0.5: small, 0.5-0.8: medium, > 0.8: large\")\n",
    "    \n",
    "    if significant_pairs:\n",
    "        print(f\"\\n   {len(significant_pairs)} significant difference(s) found:\")\n",
    "        for m1, m2, p, diff_pct in significant_pairs:\n",
    "            print(f\"     • {m1} vs {m2}: {diff_pct:+.2f}% difference (p={p:.6f})\")\n",
    "    else:\n",
    "        print(f\"\\n   No statistically significant pairwise differences found.\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'datasets': valid_datasets,\n",
    "        'levene_test': {'statistic': levene_stat, 'p_value': levene_p, 'equal_var': equal_var},\n",
    "        'omnibus_test': {'test': test_name, 'statistic': f_stat if equal_var else h_stat, \n",
    "                        'p_value': anova_p, 'significant': anova_p < alpha},\n",
    "        'pairwise_tests': pairwise_results,\n",
    "        'significant_pairs': significant_pairs,\n",
    "        'alpha': alpha,\n",
    "        'bonferroni_alpha': bonferroni_alpha\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_fairness_and_significance(analyzer, column='latency_ms', datasets=None, \n",
    "                                       fairness_metrics=None, sig_results=None, \n",
    "                                       figsize=(16, 10)):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of fairness and significance analysis.\n",
    "    \n",
    "    Args:\n",
    "        analyzer: DataAnalyzer instance\n",
    "        column: Column to analyze\n",
    "        datasets: List of datasets\n",
    "        fairness_metrics: Pre-computed fairness metrics (optional)\n",
    "        sig_results: Pre-computed significance test results (optional)\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    if datasets is None:\n",
    "        datasets = analyzer.list_datasets()\n",
    "    \n",
    "    # Compute metrics if not provided\n",
    "    if fairness_metrics is None:\n",
    "        fairness_metrics = compute_fairness_metrics(analyzer, column, datasets)         \n",
    "    \n",
    "    if sig_results is None:\n",
    "        sig_results = test_statistical_significance(analyzer, column, datasets)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
    "    fig.suptitle(f'Fairness & Significance Analysis: {column}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Mean comparison with error bars (std)\n",
    "    means = []\n",
    "    stds = []\n",
    "    for dataset in datasets:\n",
    "        df = analyzer.get_data(dataset)\n",
    "        if df is not None and column in df.columns:\n",
    "            means.append(df[column].mean())\n",
    "            stds.append(df[column].std())\n",
    "    \n",
    "    x_pos = np.arange(len(datasets))\n",
    "    axes[0, 0].bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7, color='steelblue')\n",
    "    axes[0, 0].set_xticks(x_pos)\n",
    "    axes[0, 0].set_xticklabels(datasets, rotation=45, ha='right')\n",
    "    axes[0, 0].set_ylabel(column)\n",
    "    axes[0, 0].set_title('Mean ± Std Dev')\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add horizontal line at average\n",
    "    overall_mean = np.mean(means)\n",
    "    axes[0, 0].axhline(overall_mean, color='red', linestyle='--', alpha=0.5, label='Overall Mean')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Box plot with individual points\n",
    "    data_for_box = []\n",
    "    for dataset in datasets:\n",
    "        df = analyzer.get_data(dataset)\n",
    "        if df is not None and column in df.columns:\n",
    "            data_for_box.append(df[column].values)\n",
    "    \n",
    "    bp = axes[0, 1].boxplot(data_for_box, labels=datasets, patch_artist=True)\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "    axes[0, 1].set_xticklabels(datasets, rotation=45, ha='right')\n",
    "    axes[0, 1].set_ylabel(column)\n",
    "    axes[0, 1].set_title('Distribution Comparison')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Coefficient of Variation comparison\n",
    "    cv_values = [np.std(data) / np.mean(data) for data in data_for_box]\n",
    "    axes[0, 2].bar(x_pos, cv_values, alpha=0.7, color='coral')\n",
    "    axes[0, 2].set_xticks(x_pos)\n",
    "    axes[0, 2].set_xticklabels(datasets, rotation=45, ha='right')\n",
    "    axes[0, 2].set_ylabel('Coefficient of Variation')\n",
    "    axes[0, 2].set_title('Within-Model Variability')\n",
    "    axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0, 2].axhline(0.1, color='orange', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "    axes[0, 2].axhline(0.05, color='green', linestyle='--', alpha=0.5, label='Excellent threshold')\n",
    "    axes[0, 2].legend(fontsize=8)\n",
    "    \n",
    "    # 4. Fairness metrics summary\n",
    "    axes[1, 0].axis('off')\n",
    "    fairness_text = f\"\"\"\n",
    "    FAIRNESS METRICS\n",
    "    ─────────────────────────\n",
    "    Coefficient of Variation: {fairness_metrics['coefficient_of_variation']:.4f}\n",
    "    Gini Coefficient: {fairness_metrics['gini_coefficient']:.4f}\n",
    "    Max/Min Ratio: {fairness_metrics['max_min_ratio']:.3f}x\n",
    "    Max Deviation: {fairness_metrics['max_deviation_percent']:.2f}%\n",
    "    \n",
    "    Assessment: {\"EXCELLENT\" if fairness_metrics['coefficient_of_variation'] < 0.05 \n",
    "                 else \"GOOD\" if fairness_metrics['coefficient_of_variation'] < 0.1\n",
    "                 else \"MODERATE\" if fairness_metrics['coefficient_of_variation'] < 0.2\n",
    "                 else \"POOR\"}\n",
    "    \"\"\"\n",
    "    axes[1, 0].text(0.1, 0.5, fairness_text, fontsize=10, family='monospace',\n",
    "                   verticalalignment='center')\n",
    "    \n",
    "    # 5. Pairwise p-values heatmap\n",
    "    n_datasets = len(datasets)\n",
    "    p_value_matrix = np.ones((n_datasets, n_datasets))\n",
    "    \n",
    "    for result in sig_results['pairwise_tests']:\n",
    "        i = datasets.index(result['model_1'])\n",
    "        j = datasets.index(result['model_2'])\n",
    "        p_value_matrix[i, j] = result['p_value']\n",
    "        p_value_matrix[j, i] = result['p_value']\n",
    "    \n",
    "    # Use log scale for better visualization\n",
    "    log_p_matrix = -np.log10(p_value_matrix)\n",
    "    log_p_matrix[np.isinf(log_p_matrix)] = 0  # Handle diagonal (p=1)\n",
    "    \n",
    "    im = axes[1, 1].imshow(log_p_matrix, cmap='RdYlGn_r', aspect='auto')\n",
    "    axes[1, 1].set_xticks(np.arange(n_datasets))\n",
    "    axes[1, 1].set_yticks(np.arange(n_datasets))\n",
    "    axes[1, 1].set_xticklabels(datasets, rotation=45, ha='right')\n",
    "    axes[1, 1].set_yticklabels(datasets)\n",
    "    axes[1, 1].set_title('Pairwise Significance\\n(-log10 p-value)')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=axes[1, 1])\n",
    "    cbar.set_label('-log10(p-value)', rotation=270, labelpad=15)\n",
    "    \n",
    "    # Add significance threshold line\n",
    "    bonferroni_threshold = -np.log10(sig_results['bonferroni_alpha'])\n",
    "    \n",
    "    # 6. Statistical test summary\n",
    "    axes[1, 2].axis('off')\n",
    "    sig_text = f\"\"\"\n",
    "    SIGNIFICANCE TESTS\n",
    "    ─────────────────────────\n",
    "    Test: {sig_results['omnibus_test']['test']}\n",
    "    p-value: {sig_results['omnibus_test']['p_value']:.6f}\n",
    "    Result: {\"SIGNIFICANT\" if sig_results['omnibus_test']['significant'] else \"NOT SIGNIFICANT\"}\n",
    "    \n",
    "    Bonferroni α: {sig_results['bonferroni_alpha']:.6f}\n",
    "    \n",
    "    Significant pairs: {len(sig_results['significant_pairs'])}\n",
    "    Total comparisons: {len(sig_results['pairwise_tests'])}\n",
    "    \"\"\"\n",
    "    axes[1, 2].text(0.1, 0.5, sig_text, fontsize=10, family='monospace',\n",
    "                   verticalalignment='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Enhanced statistical fairness and significance analysis functions loaded!\")\n",
    "print(\"\\nNew Features:\")\n",
    "print(\"  • Mann-Whitney U tests for robust median comparisons\")\n",
    "print(\"  • Quantile regression to analyze fairness across percentiles\")\n",
    "print(\"  • Support for mean, median, or p50-based metrics\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  # Analyze fairness using mean (default)\")\n",
    "print(\"  fairness = compute_fairness_metrics(analyzer, column='latency_ms', metric='mean')\")\n",
    "print(\"\")\n",
    "print(\"  # Analyze fairness using median/p50 (more robust to outliers)\")\n",
    "print(\"  fairness = compute_fairness_metrics(analyzer, column='latency_ms', metric='median')\")\n",
    "print(\"\")\n",
    "print(\"  # Test statistical significance\")\n",
    "print(\"  sig_results = test_statistical_significance(analyzer, column='latency_ms')\")\n",
    "print(\"\")\n",
    "print(\"  # Visualize everything\")\n",
    "print(\"  visualize_fairness_and_significance(analyzer, column='latency_ms')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r4gin9s5gfg",
   "metadata": {},
   "source": [
    "## Resource Hogging Detection\n",
    "\n",
    "This section provides tools to detect if one model is monopolizing GPU resources and starving others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eheff330i7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_model_name(dataset_name):\n",
    "    \"\"\"\n",
    "    Extract the core model name from a dataset name.\n",
    "    \n",
    "    Examples:\n",
    "        'distilgpt2_1' -> 'gpt2'\n",
    "        'solo_gpt2' -> 'gpt2'\n",
    "        'distilbert_3' -> 'bert'\n",
    "        'solo_bert' -> 'bert'\n",
    "        'llama_1' -> 'llama'\n",
    "        'solo_llama' -> 'llama'\n",
    "        'mistral7b_2' -> 'mistral'\n",
    "        'solo_mistral' -> 'mistral'\n",
    "    \"\"\"\n",
    "    name_lower = dataset_name.lower()\n",
    "    \n",
    "    # Define model name patterns\n",
    "    if 'gpt2' in name_lower or 'gpt-2' in name_lower:\n",
    "        return 'gpt2'\n",
    "    elif 'roberta' in name_lower:\n",
    "        return 'roberta'\n",
    "    elif 'bert' in name_lower:\n",
    "        return 'bert'\n",
    "    elif 'llama' in name_lower:\n",
    "        return 'llama'\n",
    "    elif 'mistral' in name_lower:\n",
    "        return 'mistral'\n",
    "    else:\n",
    "        # Fallback: return the name without numbers and underscores\n",
    "        import re\n",
    "        return re.sub(r'[_\\d]+', '', name_lower)\n",
    "\n",
    "\n",
    "def detect_resource_hogging_normalized(analyzer,\n",
    "                                        baseline_analyzers,\n",
    "                                        column='latency_ms',\n",
    "                                        datasets=None,\n",
    "                                        hogging_threshold=0.15):\n",
    "    \"\"\"\n",
    "    Detect resource hogging by comparing relative slowdown from baseline performance.\n",
    "\n",
    "    This version accounts for different model architectures having different baseline speeds.\n",
    "    Instead of comparing absolute latency, it compares how much each model slowed down\n",
    "    relative to its solo performance.\n",
    "\n",
    "    Args:\n",
    "        analyzer: DataAnalyzer with multi-tenant data\n",
    "        baseline_analyzers: DataAnalyzer or list of DataAnalyzers with solo/baseline data for each model\n",
    "                           If a list is provided, will search all analyzers for matching baseline data\n",
    "        column: Column to analyze (default 'latency_ms')\n",
    "        datasets: List of datasets (None = all)\n",
    "        hogging_threshold: Threshold for detecting hogging (default 0.15 = 15% difference in slowdown)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with normalized hogging detection results\n",
    "    \"\"\"\n",
    "    # Normalize baseline_analyzers to always be a list\n",
    "    if not isinstance(baseline_analyzers, list):\n",
    "        baseline_analyzers = [baseline_analyzers]\n",
    "    \n",
    "    if datasets is None:\n",
    "        datasets = analyzer.list_datasets()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"NORMALIZED RESOURCE HOGGING DETECTION (Baseline-Aware)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"This analysis accounts for different baseline performance of different models.\")\n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"Hogging threshold: {hogging_threshold*100:.1f}% difference in relative slowdown\")\n",
    "    print(f\"Baseline analyzers: {len(baseline_analyzers)}\")\n",
    "    \n",
    "    # Get baseline and multi-tenant stats\n",
    "    model_stats = {}\n",
    "    baseline_stats = {}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        # Multi-tenant data\n",
    "        df = analyzer.get_data(dataset)\n",
    "        if df is not None and column in df.columns:\n",
    "            model_stats[dataset] = {\n",
    "                'mean': df[column].mean(),\n",
    "                'std': df[column].std(),\n",
    "            }\n",
    "        \n",
    "        # Baseline data (try to find matching baseline from any analyzer)\n",
    "        baseline_df = None\n",
    "        matched_analyzer_idx = None\n",
    "        matched_baseline_name = None\n",
    "        \n",
    "        # Try exact match first across all analyzers\n",
    "        for idx, baseline_analyzer in enumerate(baseline_analyzers):\n",
    "            baseline_df = baseline_analyzer.get_data(dataset)\n",
    "            if baseline_df is not None:\n",
    "                matched_analyzer_idx = idx\n",
    "                matched_baseline_name = dataset\n",
    "                break\n",
    "        \n",
    "        # If no exact match, try substring match\n",
    "        if baseline_df is None:\n",
    "            for idx, baseline_analyzer in enumerate(baseline_analyzers):\n",
    "                baseline_datasets = baseline_analyzer.list_datasets()\n",
    "                for bd in baseline_datasets:\n",
    "                    if dataset in bd or bd in dataset:\n",
    "                        baseline_df = baseline_analyzer.get_data(bd)\n",
    "                        matched_analyzer_idx = idx\n",
    "                        matched_baseline_name = bd\n",
    "                        break\n",
    "                if baseline_df is not None:\n",
    "                    break\n",
    "        \n",
    "        # If still no match, try model name extraction\n",
    "        if baseline_df is None:\n",
    "            dataset_model = _extract_model_name(dataset)\n",
    "            for idx, baseline_analyzer in enumerate(baseline_analyzers):\n",
    "                baseline_datasets = baseline_analyzer.list_datasets()\n",
    "                for bd in baseline_datasets:\n",
    "                    bd_model = _extract_model_name(bd)\n",
    "                    if dataset_model == bd_model:\n",
    "                        baseline_df = baseline_analyzer.get_data(bd)\n",
    "                        matched_analyzer_idx = idx\n",
    "                        matched_baseline_name = bd\n",
    "                        print(f\"   Matched '{dataset}' -> '{bd}' via model name '{dataset_model}'\")\n",
    "                        break\n",
    "                if baseline_df is not None:\n",
    "                    break\n",
    "        \n",
    "        if baseline_df is not None and column in baseline_df.columns:\n",
    "            baseline_stats[dataset] = {\n",
    "                'mean': baseline_df[column].mean(),\n",
    "                'std': baseline_df[column].std(),\n",
    "                'analyzer_idx': matched_analyzer_idx,\n",
    "                'baseline_name': matched_baseline_name\n",
    "            }\n",
    "    \n",
    "    # Check if we have baseline data\n",
    "    missing_baseline = [d for d in datasets if d not in baseline_stats]\n",
    "    if missing_baseline:\n",
    "        print(f\"\\n  WARNING: Missing baseline data for: {', '.join(missing_baseline)}\")\n",
    "        all_baseline_datasets = []\n",
    "        for ba in baseline_analyzers:\n",
    "            all_baseline_datasets.extend(ba.list_datasets())\n",
    "        print(f\"   Available baseline datasets: {all_baseline_datasets}\")\n",
    "        print(f\"   Falling back to absolute comparison for models without baseline.\\n\")\n",
    "    \n",
    "    # Calculate relative slowdown for each model\n",
    "    print(f\"\\n{'Model':<20s} {'Baseline':>10s} {'Multi':>10s} {'Slowdown':>10s} {'Status':<25s}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    slowdowns = {}\n",
    "    hogging_models = []\n",
    "    starved_models = []\n",
    "    fair_models = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        if dataset not in model_stats:\n",
    "            continue\n",
    "            \n",
    "        multi_mean = model_stats[dataset]['mean']\n",
    "        \n",
    "        if dataset in baseline_stats:\n",
    "            baseline_mean = baseline_stats[dataset]['mean']\n",
    "            \n",
    "            if column == 'latency_ms':\n",
    "                # Slowdown = (multi_latency - baseline_latency) / baseline_latency\n",
    "                # Positive = slower (worse), Negative = faster (better, suspicious!)\n",
    "                slowdown = (multi_mean - baseline_mean) / baseline_mean\n",
    "            elif column == 'throughput':\n",
    "                # Slowdown = (baseline_throughput - multi_throughput) / baseline_throughput\n",
    "                # Positive = worse throughput, Negative = better (suspicious!)\n",
    "                slowdown = (baseline_mean - multi_mean) / baseline_mean\n",
    "            \n",
    "            slowdowns[dataset] = slowdown\n",
    "        else:\n",
    "            # No baseline - can't calculate relative slowdown\n",
    "            slowdowns[dataset] = None\n",
    "            baseline_mean = None\n",
    "    \n",
    "    # Calculate average slowdown (only for models with baseline)\n",
    "    valid_slowdowns = [s for s in slowdowns.values() if s is not None]\n",
    "    if len(valid_slowdowns) == 0:\n",
    "        print(\"ERROR: No baseline data available for any models!\")\n",
    "        return None\n",
    "    \n",
    "    avg_slowdown = np.mean(valid_slowdowns)\n",
    "    \n",
    "    # Detect hogging based on relative slowdown differences\n",
    "    for dataset in datasets:\n",
    "        if dataset not in model_stats:\n",
    "            continue\n",
    "        \n",
    "        multi_mean = model_stats[dataset]['mean']\n",
    "        baseline_mean = baseline_stats.get(dataset, {}).get('mean', None)\n",
    "        slowdown = slowdowns[dataset]\n",
    "        \n",
    "        if slowdown is None:\n",
    "            status = \"No baseline data\"\n",
    "            print(f\"{dataset:<20s} {'N/A':>10s} {multi_mean:10.2f} {'N/A':>10s} {status:<25s}\")\n",
    "            continue\n",
    "        \n",
    "        # Compare this model's slowdown to the average slowdown\n",
    "        slowdown_diff = slowdown - avg_slowdown\n",
    "        \n",
    "        # Determine status\n",
    "        if abs(slowdown_diff) > hogging_threshold:\n",
    "            if slowdown_diff < 0:\n",
    "                # This model slowed down LESS than average = getting priority\n",
    "                status = f\"HOGGING ({slowdown_diff*100:+.1f}%)\"\n",
    "                hogging_models.append((dataset, slowdown_diff * 100, slowdown * 100))\n",
    "            else:\n",
    "                # This model slowed down MORE than average = being starved\n",
    "                status = f\"STARVED ({slowdown_diff*100:+.1f}%)\"\n",
    "                starved_models.append((dataset, slowdown_diff * 100, slowdown * 100))\n",
    "        else:\n",
    "            status = f\"✓ Fair ({slowdown_diff*100:+.1f}%)\"\n",
    "            fair_models.append(dataset)\n",
    "        \n",
    "        print(f\"{dataset:<20s} {baseline_mean:10.2f} {multi_mean:10.2f} {slowdown*100:+9.1f}% {status:<25s}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ANALYSIS:\")\n",
    "    print(f\"  Average slowdown: {avg_slowdown*100:+.1f}%\")\n",
    "    print(f\"  (This is expected overhead from multi-tenancy)\")\n",
    "    print(f\"\\n  Models with FAIR resource allocation: {len(fair_models)}\")\n",
    "    \n",
    "    if hogging_models:\n",
    "        print(f\"\\n  RESOURCE HOGGING DETECTED: {len(hogging_models)} model(s)\")\n",
    "        for model, diff, total_slowdown in hogging_models:\n",
    "            print(f\"     • {model}:\")\n",
    "            print(f\"       - Slowdown: {total_slowdown:+.1f}% (vs avg {avg_slowdown*100:+.1f}%)\")\n",
    "            print(f\"       - Difference: {diff:+.1f}% LESS slowdown than others\")\n",
    "            print(f\"       → Getting PRIORITY access to GPU resources\")\n",
    "    \n",
    "    if starved_models:\n",
    "        print(f\"\\n RESOURCE STARVATION DETECTED: {len(starved_models)} model(s)\")\n",
    "        for model, diff, total_slowdown in starved_models:\n",
    "            print(f\"     • {model}:\")\n",
    "            print(f\"       - Slowdown: {total_slowdown:+.1f}% (vs avg {avg_slowdown*100:+.1f}%)\")\n",
    "            print(f\"       - Difference: {diff:+.1f}% MORE slowdown than others\")\n",
    "            print(f\"       → Being STARVED of GPU resources\")\n",
    "    \n",
    "    if not hogging_models and not starved_models:\n",
    "        print(f\"\\n  NO HOGGING DETECTED - All models experience similar slowdown!\")\n",
    "        print(f\"    Resources are shared fairly across different model types.\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. Baseline vs Multi-tenant comparison\n",
    "    model_names = [d for d in datasets if d in model_stats and d in baseline_stats]\n",
    "    baseline_means = [baseline_stats[m]['mean'] for m in model_names]\n",
    "    multi_means = [model_stats[m]['mean'] for m in model_names]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0].bar(x - width/2, baseline_means, width, label='Baseline (solo)', alpha=0.7, color='green')\n",
    "    axes[0].bar(x + width/2, multi_means, width, label='Multi-tenant', alpha=0.7, color='blue')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    axes[0].set_ylabel(column)\n",
    "    axes[0].set_title('Baseline vs Multi-tenant Performance')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 2. Slowdown comparison\n",
    "    slowdown_values = [slowdowns[m] * 100 for m in model_names]\n",
    "    colors = []\n",
    "    for m in model_names:\n",
    "        slowdown_diff = slowdowns[m] - avg_slowdown\n",
    "        if abs(slowdown_diff) > hogging_threshold:\n",
    "            colors.append('red' if slowdown_diff < 0 else 'orange')\n",
    "        else:\n",
    "            colors.append('green')\n",
    "    \n",
    "    axes[1].bar(range(len(model_names)), slowdown_values, color=colors, alpha=0.7)\n",
    "    axes[1].axhline(avg_slowdown * 100, color='blue', linestyle='--', linewidth=2, label=f'Avg slowdown ({avg_slowdown*100:.1f}%)')\n",
    "    axes[1].set_xticks(range(len(model_names)))\n",
    "    axes[1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    axes[1].set_ylabel('Slowdown from Baseline (%)')\n",
    "    axes[1].set_title('Relative Slowdown per Model')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Deviation from average slowdown\n",
    "    deviations = [(slowdowns[m] - avg_slowdown) * 100 for m in model_names]\n",
    "    axes[2].bar(range(len(model_names)), deviations, color=colors, alpha=0.7)\n",
    "    axes[2].axhline(0, color='blue', linestyle='-', linewidth=1)\n",
    "    axes[2].axhline(hogging_threshold * 100, color='red', linestyle='--', alpha=0.5, label='Hogging threshold')\n",
    "    axes[2].axhline(-hogging_threshold * 100, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[2].set_xticks(range(len(model_names)))\n",
    "    axes[2].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    axes[2].set_ylabel('Deviation from Avg Slowdown (%)')\n",
    "    axes[2].set_title('Hogging Detection (Normalized)')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'hogging_models': hogging_models,\n",
    "        'starved_models': starved_models,\n",
    "        'fair_models': fair_models,\n",
    "        'average_slowdown': avg_slowdown,\n",
    "        'slowdowns': slowdowns,\n",
    "        'model_stats': model_stats,\n",
    "        'baseline_stats': baseline_stats\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Normalized (baseline-aware) resource hogging detection function loaded!\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  # Compare multi-tenant to baseline performance (single baseline analyzer)\")\n",
    "print(\"  hogging = detect_resource_hogging_normalized(\")\n",
    "print(\"      analyzer=triple_analyzer,           # Multi-tenant data\")\n",
    "print(\"      baseline_analyzers=single_analyzer, # Single baseline analyzer\")\n",
    "print(\"      column='latency_ms',\")\n",
    "print(\"      hogging_threshold=0.15\")\n",
    "print(\"  )\")\n",
    "print(\"\")\n",
    "print(\"  # Compare multi-tenant to multiple baseline analyzers\")\n",
    "print(\"  hogging = detect_resource_hogging_normalized(\")\n",
    "print(\"      analyzer=triple_analyzer,\")\n",
    "print(\"      baseline_analyzers=[baseline1, baseline2, baseline3],  # List of baseline analyzers\")\n",
    "print(\"      column='latency_ms',\")\n",
    "print(\"      hogging_threshold=0.15\")\n",
    "print(\"  )\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
